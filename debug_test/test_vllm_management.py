#!/usr/bin/env python3
"""VLLMæ¨¡å‹æœåŠ¡ç®¡ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬"""

import sys
import os
import asyncio
import json
import requests
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config import AiPlatformConfig
from app.services.vllm_service import get_vllm_manager
from app.services.ssh_manager import get_ssh_manager

class VllmTester:
    """VLLMåŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config = AiPlatformConfig()
        self.vllm_manager = get_vllm_manager(self.config)
        self.ssh_manager = get_ssh_manager()
        self.base_url = f"http://{self.config.server_host}:{self.config.server_port}"
        
    def test_server_diagnosis(self, server_name: str):
        """æµ‹è¯•æœåŠ¡å™¨ç¯å¢ƒè¯Šæ–­"""
        print(f"\n=== æµ‹è¯•æœåŠ¡å™¨ç¯å¢ƒè¯Šæ–­: {server_name} ===")
        
        try:
            # è·å–æœåŠ¡å™¨é…ç½®
            server_config = None
            for server in self.config.gpu_servers:
                if server.name == server_name:
                    server_config = server
                    break
            
            if not server_config:
                print(f"âŒ æœåŠ¡å™¨ '{server_name}' ä¸å­˜åœ¨")
                return False
            
            # æ‰§è¡Œè¯Šæ–­
            diagnosis = self.vllm_manager.diagnose_server_environment(server_config)
            
            print("ğŸ“‹ è¯Šæ–­ç»“æœ:")
            print(f"  SSHè¿æ¥: {'âœ…' if diagnosis['ssh_connection'] else 'âŒ'}")
            print(f"  Pythonç‰ˆæœ¬: {diagnosis.get('python_version', 'æœªæ£€æµ‹åˆ°')}")
            print(f"  VLLMå®‰è£…: {'âœ…' if diagnosis['vllm_installed'] else 'âŒ'}")
            print(f"  GPUå¯ç”¨: {'âœ…' if diagnosis['gpu_available'] else 'âŒ'}")
            print(f"  NVIDIA-SMI: {'âœ…' if diagnosis['nvidia_smi'] else 'âŒ'}")
            print(f"  æ¨¡å‹è·¯å¾„: {'âœ…' if diagnosis['model_path_exists'] else 'âŒ'}")
            
            if diagnosis['errors']:
                print("âš ï¸  å‘ç°çš„é—®é¢˜:")
                for error in diagnosis['errors']:
                    print(f"    â€¢ {error}")
            
            if diagnosis['suggestions']:
                print("ğŸ’¡ ä¿®å¤å»ºè®®:")
                for suggestion in diagnosis['suggestions']:
                    print(f"    â€¢ {suggestion}")
                    
            return len(diagnosis['errors']) == 0
            
        except Exception as e:
            print(f"âŒ è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def test_api_diagnosis(self, server_name: str):
        """æµ‹è¯•APIè¯Šæ–­æ¥å£"""
        print(f"\n=== æµ‹è¯•APIè¯Šæ–­æ¥å£: {server_name} ===")
        
        try:
            url = f"{self.base_url}/api/vllm/diagnosis/{server_name}"
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if data['success']:
                    print("âœ… APIè¯Šæ–­æ¥å£æ­£å¸¸")
                    diagnosis = data['data']
                    print(f"  SSHè¿æ¥: {'âœ…' if diagnosis['ssh_connection'] else 'âŒ'}")
                    print(f"  VLLMå®‰è£…: {'âœ…' if diagnosis['vllm_installed'] else 'âŒ'}")
                    return True
                else:
                    print(f"âŒ APIè¿”å›å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ APIæµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def test_model_discovery(self, server_name: str):
        """æµ‹è¯•æ¨¡å‹å‘ç°åŠŸèƒ½"""
        print(f"\n=== æµ‹è¯•æ¨¡å‹å‘ç°åŠŸèƒ½: {server_name} ===")
        
        try:
            url = f"{self.base_url}/api/vllm/models/{server_name}"
            response = requests.get(url, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                if data['success']:
                    models = data['data']['discovered_models']
                    print(f"âœ… å‘ç° {len(models)} ä¸ªæ¨¡å‹:")
                    for model in models[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        print(f"  ğŸ“ {model['name']} ({model['size']})")
                        print(f"     è·¯å¾„: {model['path']}")
                    
                    if len(models) > 5:
                        print(f"  ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")
                        
                    return len(models) > 0
                else:
                    print(f"âŒ æ¨¡å‹å‘ç°å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âŒ æ¨¡å‹å‘ç°è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹å‘ç°æµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def test_port_check(self, server_name: str):
        """æµ‹è¯•ç«¯å£æ£€æŸ¥åŠŸèƒ½"""
        print(f"\n=== æµ‹è¯•ç«¯å£æ£€æŸ¥åŠŸèƒ½: {server_name} ===")
        
        try:
            url = f"{self.base_url}/api/vllm/ports/{server_name}?start_port=8000&count=5"
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if data['success']:
                    available_ports = data['data']['available_ports']
                    print(f"âœ… æ£€æŸ¥ç«¯å£8000-8004ï¼Œ{len(available_ports)}ä¸ªå¯ç”¨")
                    print(f"  å¯ç”¨ç«¯å£: {available_ports}")
                    return True
                else:
                    print(f"âŒ ç«¯å£æ£€æŸ¥å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âŒ ç«¯å£æ£€æŸ¥è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ ç«¯å£æ£€æŸ¥æµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def test_service_listing(self, server_name: str):
        """æµ‹è¯•æœåŠ¡åˆ—è¡¨åŠŸèƒ½"""
        print(f"\n=== æµ‹è¯•æœåŠ¡åˆ—è¡¨åŠŸèƒ½: {server_name} ===")
        
        try:
            url = f"{self.base_url}/api/vllm/list/{server_name}"
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if data['success']:
                    services = data['data']['services']
                    print(f"âœ… å½“å‰è¿è¡Œ {len(services)} ä¸ªVLLMæœåŠ¡")
                    
                    for service in services:
                        print(f"  ğŸŸ¢ PID: {service['pid']}, ç«¯å£: {service.get('port', 'æœªçŸ¥')}")
                        print(f"     CPU: {service['cpu_usage']}%, å†…å­˜: {service['memory_usage']}%")
                    
                    return True
                else:
                    print(f"âŒ æœåŠ¡åˆ—è¡¨è·å–å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âŒ æœåŠ¡åˆ—è¡¨è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æœåŠ¡åˆ—è¡¨æµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def test_vllm_start_stop(self, server_name: str, model_path: str = None):
        """æµ‹è¯•VLLMæœåŠ¡å¯åŠ¨å’Œåœæ­¢"""
        print(f"\n=== æµ‹è¯•VLLMæœåŠ¡å¯åŠ¨åœæ­¢: {server_name} ===")
        
        if not model_path:
            print("âš ï¸  æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡å¯åŠ¨åœæ­¢æµ‹è¯•")
            return True
        
        test_port = 8001
        
        try:
            # 1. å¯åŠ¨æœåŠ¡
            print("ğŸš€ å¯åŠ¨VLLMæœåŠ¡...")
            start_data = {
                "server_name": server_name,
                "model_path": model_path,
                "port": test_port,
                "gpu_indices": "0",
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.8,
                "tensor_parallel_size": 1
            }
            
            response = requests.post(
                f"{self.base_url}/api/vllm/start",
                json=start_data,
                timeout=90
            )
            
            if response.status_code != 200:
                print(f"âŒ å¯åŠ¨è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return False
            
            start_result = response.json()
            if not start_result['success']:
                print(f"âŒ å¯åŠ¨å¤±è´¥: {start_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
            
            pid = start_result['data']['pid']
            print(f"âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ, PID: {pid}, ç«¯å£: {test_port}")
            
            # 2. ç­‰å¾…ä¸€æ®µæ—¶é—´
            print("â±ï¸  ç­‰å¾…æœåŠ¡ç¨³å®š...")
            time.sleep(10)
            
            # 3. æ£€æŸ¥æœåŠ¡çŠ¶æ€
            print("ğŸ” æ£€æŸ¥æœåŠ¡çŠ¶æ€...")
            status_response = requests.get(
                f"{self.base_url}/api/vllm/status/{server_name}?pid={pid}&port={test_port}",
                timeout=30
            )
            
            if status_response.status_code == 200:
                status_data = status_response.json()
                if status_data['success']:
                    status = status_data['data']['status']
                    print(f"  è¿è¡ŒçŠ¶æ€: {'âœ…' if status['running'] else 'âŒ'}")
                    print(f"  è¿›ç¨‹å­˜åœ¨: {'âœ…' if status['pid_exists'] else 'âŒ'}")
                    print(f"  ç«¯å£å ç”¨: {'âœ…' if status['port_in_use'] else 'âŒ'}")
            
            # 4. åœæ­¢æœåŠ¡
            print("ğŸ›‘ åœæ­¢VLLMæœåŠ¡...")
            stop_data = {
                "server_name": server_name,
                "pid": pid
            }
            
            stop_response = requests.post(
                f"{self.base_url}/api/vllm/stop",
                json=stop_data,
                timeout=30
            )
            
            if stop_response.status_code == 200:
                stop_result = stop_response.json()
                if stop_result['success']:
                    print("âœ… æœåŠ¡åœæ­¢æˆåŠŸ")
                    return True
                else:
                    print(f"âŒ åœæ­¢å¤±è´¥: {stop_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âŒ åœæ­¢è¯·æ±‚å¤±è´¥: HTTP {stop_response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ å¯åŠ¨åœæ­¢æµ‹è¯•å‡ºé”™: {e}")
            return False
    
    def run_all_tests(self, server_name: str, model_path: str = None):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print(f"ğŸ§ª å¼€å§‹VLLMç®¡ç†åŠŸèƒ½æµ‹è¯• - æœåŠ¡å™¨: {server_name}")
        print("=" * 50)
        
        tests = [
            ("æœåŠ¡å™¨ç¯å¢ƒè¯Šæ–­", lambda: self.test_server_diagnosis(server_name)),
            ("APIè¯Šæ–­æ¥å£", lambda: self.test_api_diagnosis(server_name)),
            ("æ¨¡å‹å‘ç°åŠŸèƒ½", lambda: self.test_model_discovery(server_name)),
            ("ç«¯å£æ£€æŸ¥åŠŸèƒ½", lambda: self.test_port_check(server_name)),
            ("æœåŠ¡åˆ—è¡¨åŠŸèƒ½", lambda: self.test_service_listing(server_name)),
        ]
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œæ·»åŠ å¯åŠ¨åœæ­¢æµ‹è¯•
        if model_path:
            tests.append(("VLLMå¯åŠ¨åœæ­¢", lambda: self.test_vllm_start_stop(server_name, model_path)))
        
        passed = 0
        total = len(tests)
        
        for test_name, test_func in tests:
            try:
                if test_func():
                    passed += 1
                    print(f"âœ… {test_name} - é€šè¿‡")
                else:
                    print(f"âŒ {test_name} - å¤±è´¥")
            except Exception as e:
                print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
        
        print("\n" + "=" * 50)
        print(f"ğŸ† æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
        
        if passed == total:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼VLLMç®¡ç†åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®å’Œç¯å¢ƒ")
        
        return passed == total

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python test_vllm_management.py <æœåŠ¡å™¨åç§°> [æ¨¡å‹è·¯å¾„]")
        print("ç¤ºä¾‹: python test_vllm_management.py GPU-Server-1 /path/to/model")
        return
    
    server_name = sys.argv[1]
    model_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    tester = VllmTester()
    tester.run_all_tests(server_name, model_path)

if __name__ == "__main__":
    main() 