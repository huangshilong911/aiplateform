#!/usr/bin/env python3
"""VLLMç¯å¢ƒå¿«é€Ÿè¯Šæ–­è„šæœ¬"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config import AiPlatformConfig
from app.services.vllm_service import get_vllm_manager

def print_banner():
    """æ‰“å°è¯Šæ–­æ¨ªå¹…"""
    print("=" * 60)
    print("ğŸ” VLLMç¯å¢ƒå¿«é€Ÿè¯Šæ–­å·¥å…·")
    print("=" * 60)

def diagnose_server(server_name: str):
    """è¯Šæ–­æŒ‡å®šæœåŠ¡å™¨"""
    print(f"\nğŸ“¡ æ­£åœ¨è¯Šæ–­æœåŠ¡å™¨: {server_name}")
    print("-" * 40)
    
    try:
        config = AiPlatformConfig()
        vllm_manager = get_vllm_manager(config)
        
        # è·å–æœåŠ¡å™¨é…ç½®
        server_config = None
        for server in config.gpu_servers:
            if server.name == server_name:
                server_config = server
                break
        
        if not server_config:
            print(f"âŒ é”™è¯¯: æœåŠ¡å™¨ '{server_name}' åœ¨é…ç½®ä¸­ä¸å­˜åœ¨")
            print("\nğŸ’¡ å¯ç”¨çš„æœåŠ¡å™¨:")
            for server in config.gpu_servers:
                status = "âœ… å¯ç”¨" if server.enabled else "âŒ ç¦ç”¨"
                print(f"   â€¢ {server.name} ({server.host}) - {status}")
            return False
        
        if not server_config.enabled:
            print(f"âš ï¸  è­¦å‘Š: æœåŠ¡å™¨ '{server_name}' å·²ç¦ç”¨")
            return False
        
        # æ‰§è¡Œè¯Šæ–­
        print("ğŸ” å¼€å§‹ç¯å¢ƒæ£€æµ‹...")
        diagnosis = vllm_manager.diagnose_server_environment(server_config)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“‹ è¯Šæ–­ç»“æœ:")
        
        items = [
            ("SSHè¿æ¥", diagnosis['ssh_connection']),
            ("Pythonç¯å¢ƒ", diagnosis['python_version'] is not None),
            ("VLLMå®‰è£…", diagnosis['vllm_installed']),
            ("GPUå¯ç”¨æ€§", diagnosis['gpu_available']),
            ("NVIDIAé©±åŠ¨", diagnosis['nvidia_smi']),
            ("æ¨¡å‹è·¯å¾„", diagnosis['model_path_exists'])
        ]
        
        all_good = True
        for name, status in items:
            icon = "âœ…" if status else "âŒ"
            print(f"  {icon} {name}")
            if not status:
                all_good = False
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if diagnosis['python_version']:
            print(f"\nğŸ Pythonç‰ˆæœ¬: {diagnosis['python_version']}")
        
        if diagnosis['vllm_installed'] and 'vllm_version' in diagnosis:
            print(f"ğŸš€ VLLMç‰ˆæœ¬: {diagnosis['vllm_version']}")
        
        if diagnosis['gpu_info']:
            gpu_info = diagnosis['gpu_info']
            if gpu_info.get('driver_version'):
                print(f"ğŸ® NVIDIAé©±åŠ¨: {gpu_info['driver_version']}")
            if gpu_info.get('cuda_version'):
                print(f"âš¡ CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
            if gpu_info.get('gpu_count'):
                print(f"ğŸ”¢ GPUæ•°é‡: {gpu_info['gpu_count']}")
        
        # æ˜¾ç¤ºé”™è¯¯å’Œå»ºè®®
        if diagnosis['errors']:
            print(f"\nâš ï¸  å‘ç° {len(diagnosis['errors'])} ä¸ªé—®é¢˜:")
            for i, error in enumerate(diagnosis['errors'], 1):
                print(f"   {i}. {error}")
        
        if diagnosis['suggestions']:
            print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
            for i, suggestion in enumerate(diagnosis['suggestions'], 1):
                print(f"   {i}. {suggestion}")
        
        # æ€»ç»“
        print("\n" + "=" * 40)
        if all_good:
            print("ğŸ‰ ç¯å¢ƒæ£€æµ‹é€šè¿‡ï¼å¯ä»¥æ­£å¸¸ä½¿ç”¨VLLMæœåŠ¡")
        else:
            print("âš ï¸  ç¯å¢ƒå­˜åœ¨é—®é¢˜ï¼Œè¯·æ ¹æ®ä¸Šè¿°å»ºè®®è¿›è¡Œä¿®å¤")
        
        return all_good
        
    except Exception as e:
        print(f"âŒ è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def diagnose_all_servers():
    """è¯Šæ–­æ‰€æœ‰æœåŠ¡å™¨"""
    try:
        config = AiPlatformConfig()
        enabled_servers = [s for s in config.gpu_servers if s.enabled]
        
        if not enabled_servers:
            print("âŒ æœªæ‰¾åˆ°å·²å¯ç”¨çš„GPUæœåŠ¡å™¨")
            return False
        
        print(f"ğŸ“¡ å‘ç° {len(enabled_servers)} ä¸ªå·²å¯ç”¨çš„æœåŠ¡å™¨")
        
        results = {}
        for server in enabled_servers:
            results[server.name] = diagnose_server(server.name)
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯Šæ–­æ±‡æ€»:")
        print("-" * 40)
        
        healthy_count = 0
        for server_name, is_healthy in results.items():
            status = "âœ… æ­£å¸¸" if is_healthy else "âŒ å¼‚å¸¸"
            print(f"  â€¢ {server_name}: {status}")
            if is_healthy:
                healthy_count += 1
        
        print(f"\nğŸ† å¥åº·æœåŠ¡å™¨: {healthy_count}/{len(enabled_servers)}")
        
        if healthy_count == len(enabled_servers):
            print("ğŸ‰ æ‰€æœ‰æœåŠ¡å™¨ç¯å¢ƒæ­£å¸¸ï¼")
        elif healthy_count > 0:
            print("âš ï¸  éƒ¨åˆ†æœåŠ¡å™¨éœ€è¦ä¿®å¤")
        else:
            print("âŒ æ‰€æœ‰æœåŠ¡å™¨éƒ½å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥é…ç½®")
        
        return healthy_count > 0
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯Šæ–­å‡ºé”™: {e}")
        return False

def show_config_info():
    """æ˜¾ç¤ºé…ç½®ä¿¡æ¯"""
    try:
        config = AiPlatformConfig()
        
        print("\nâš™ï¸  é…ç½®ä¿¡æ¯:")
        print("-" * 40)
        
        print(f"ğŸŒ æœåŠ¡å™¨åœ°å€: {config.server_host}:{config.server_port}")
        print(f"ğŸ“Š æ•°æ®åº“: {config.database_url}")
        print(f"ğŸ”Œ VLLMç«¯å£èŒƒå›´: {config.vllm.default_port_range.start}-{config.vllm.default_port_range.end}")
        print(f"ğŸ’¾ GPUå†…å­˜åˆ©ç”¨ç‡: {config.vllm.default_gpu_memory_utilization}")
        print(f"ğŸ“ é»˜è®¤æ¨¡å‹é•¿åº¦: {config.vllm.default_max_model_len}")
        
        print(f"\nğŸ–¥ï¸  é…ç½®çš„GPUæœåŠ¡å™¨ ({len(config.gpu_servers)} ä¸ª):")
        for server in config.gpu_servers:
            status = "âœ… å¯ç”¨" if server.enabled else "âŒ ç¦ç”¨"
            print(f"  â€¢ {server.name}")
            print(f"    åœ°å€: {server.host}:{server.port}")
            print(f"    ç”¨æˆ·: {server.username}")
            print(f"    GPUæ•°é‡: {server.gpu_count}")
            print(f"    æ¨¡å‹è·¯å¾„: {server.model_path}")
            print(f"    çŠ¶æ€: {status}")
            print()
    
    except Exception as e:
        print(f"âŒ è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {e}")

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("""
ğŸ“š ä½¿ç”¨è¯´æ˜:

ğŸ” è¯Šæ–­æŒ‡å®šæœåŠ¡å™¨:
   python quick_diagnosis.py <æœåŠ¡å™¨åç§°>
   
ğŸ” è¯Šæ–­æ‰€æœ‰æœåŠ¡å™¨:
   python quick_diagnosis.py --all
   
âš™ï¸  æ˜¾ç¤ºé…ç½®ä¿¡æ¯:
   python quick_diagnosis.py --config
   
ğŸ“š æ˜¾ç¤ºå¸®åŠ©:
   python quick_diagnosis.py --help

ğŸ’¡ ç¤ºä¾‹:
   python quick_diagnosis.py GPU-Server-1
   python quick_diagnosis.py --all
   python quick_diagnosis.py --config
   
ğŸš¨ å¸¸è§é—®é¢˜è§£å†³:
   1. SSHè¿æ¥å¤±è´¥ -> æ£€æŸ¥æœåŠ¡å™¨åœ°å€ã€ç«¯å£ã€ç”¨æˆ·åå’Œå¯†ç 
   2. Pythonæœªå®‰è£… -> åœ¨è¿œç¨‹æœåŠ¡å™¨å®‰è£…Python 3.8+
   3. VLLMæœªå®‰è£… -> æ‰§è¡Œ: pip install vllm
   4. GPUä¸å¯ç”¨ -> æ£€æŸ¥NVIDIAé©±åŠ¨ç¨‹åºå®‰è£…
   5. æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ -> åˆ›å»ºç›®å½•æˆ–æ£€æŸ¥è·¯å¾„é…ç½®
""")

def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    if len(sys.argv) < 2:
        show_help()
        return
    
    arg = sys.argv[1]
    
    if arg == "--help" or arg == "-h":
        show_help()
    elif arg == "--all":
        diagnose_all_servers()
    elif arg == "--config":
        show_config_info()
    else:
        # è¯Šæ–­æŒ‡å®šæœåŠ¡å™¨
        server_name = arg
        diagnose_server(server_name)

if __name__ == "__main__":
    main() 