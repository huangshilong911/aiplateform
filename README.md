# AIå¹³å°ç®¡ç†ç³»ç»Ÿ (Pythonç‰ˆ)

åŸºäºFastAPIæ„å»ºçš„ä¼ä¸šçº§GPUæœåŠ¡å™¨ç›‘æ§å’Œå¤§æ¨¡å‹æœåŠ¡ç®¡ç†å¹³å°ï¼Œæä¾›å®æ—¶ç›‘æ§ã€æ¨¡å‹ç®¡ç†ã€SSHè¿œç¨‹æ§åˆ¶ã€WebSocketç»ˆç«¯ã€VLLMæœåŠ¡ç®¡ç†ç­‰å…¨æ–¹ä½åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### éœ€æ±‚1: GPUæœåŠ¡å™¨ç›‘æ§
- âœ… å®æ—¶ç›‘æ§GPUä½¿ç”¨ç‡ã€å†…å­˜ã€æ¸©åº¦ã€åŠŸè€—ç­‰æŒ‡æ ‡
- âœ… æ”¯æŒå¤šæœåŠ¡å™¨åŒæ—¶ç›‘æ§
- âœ… å†å²æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢
- âœ… GPUçŠ¶æ€åˆ†æå’Œç»Ÿè®¡
- âœ… WebSocketå®æ—¶æ¨é€GPUæ•°æ®

### éœ€æ±‚2: æ¨¡å‹åˆ—è¡¨ç®¡ç†
- âœ… è‡ªåŠ¨å‘ç°æœåŠ¡å™¨ä¸Šçš„å¤§æ¨¡å‹æ–‡ä»¶
- âœ… æ¨¡å‹ä¿¡æ¯å±•ç¤ºï¼ˆè·¯å¾„ã€å¤§å°ã€ç±»å‹ç­‰ï¼‰
- âœ… æ¨¡å‹åˆ†ç±»å’Œç»„ç»‡ç®¡ç†
- âœ… æ‰¹é‡æ¨¡å‹æ“ä½œ
- âœ… ModelScopeæ¨¡å‹ä¸‹è½½æ”¯æŒ

### éœ€æ±‚3: æ¨¡å‹æœåŠ¡æ§åˆ¶
- âœ… åœ¨çº¿å¯åŠ¨/åœæ­¢å¤§æ¨¡å‹æœåŠ¡
- âœ… æ”¯æŒVLLMæ¡†æ¶
- âœ… æœåŠ¡çŠ¶æ€ç›‘æ§
- âœ… ç«¯å£ç®¡ç†å’Œèµ„æºåˆ†é…
- âœ… æœåŠ¡æ—¥å¿—æŸ¥çœ‹
- âœ… åŠ¨æ€ç«¯å£èŒƒå›´é…ç½®

### éœ€æ±‚3.1: VLLMä¸“ä¸šç®¡ç†ï¼ˆæ–°åŠŸèƒ½ï¼‰
- âœ… ç‹¬ç«‹VLLMç®¡ç†é¡µé¢
- âœ… å®Œæ•´çš„VLLMå‚æ•°é…ç½®ï¼ˆåŸºç¡€+é«˜çº§ï¼‰
- âœ… æ™ºèƒ½å‚æ•°é¢„è®¾ï¼ˆå°/ä¸­/å¤§æ¨¡å‹ï¼‰
- âœ… é…ç½®æ¨¡æ¿ä¿å­˜å’ŒåŠ è½½
- âœ… å®æ—¶æ€§èƒ½ç›‘æ§
- âœ… å¥åº·æ£€æŸ¥å’Œè¿æ¥æµ‹è¯•
- âœ… æ‰¹é‡æœåŠ¡æ“ä½œ
- âœ… å¢å¼ºçš„ç¯å¢ƒè¯Šæ–­

### éœ€æ±‚4: SSHè¿œç¨‹ç™»å½•
- âœ… å®‰å…¨çš„SSHè¿æ¥ç®¡ç†
- âœ… è¿œç¨‹å‘½ä»¤æ‰§è¡Œ
- âœ… è¿æ¥çŠ¶æ€ç›‘æ§
- âœ… å¤šæœåŠ¡å™¨ä¼šè¯ç®¡ç†
- âœ… **WebSocketå®æ—¶ç»ˆç«¯**ï¼ˆæ–°åŠŸèƒ½ï¼‰
- âœ… äº¤äº’å¼SSHä¼šè¯

### éœ€æ±‚5: é…ç½®ç®¡ç†
- âœ… åŠ¨æ€æ·»åŠ /ä¿®æ”¹/åˆ é™¤GPUæœåŠ¡å™¨
- âœ… è¿æ¥é…ç½®æµ‹è¯•
- âœ… ç³»ç»Ÿå‚æ•°é…ç½®
- âœ… é…ç½®æ–‡ä»¶ç®¡ç†
- âœ… å®æ—¶é…ç½®æ›´æ–°

### éœ€æ±‚6: å¼€å‘è€…å·¥å…·ï¼ˆæ–°åŠŸèƒ½ï¼‰
- âœ… APIæ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆï¼ˆSwagger/ReDocï¼‰
- âœ… åŸå§‹æ•°æ®APIè®¿é—®
- âœ… ç³»ç»ŸçŠ¶æ€è¯Šæ–­å·¥å…·
- âœ… æ•…éšœæ’æŸ¥å·¥å…·
- âœ… å®æ—¶æ—¥å¿—æŸ¥çœ‹
- âœ… å¢å¼ºçš„GPUç¯å¢ƒè¯Šæ–­
- âœ… Pythonç¯å¢ƒæ£€æµ‹
- âœ… CUDAè¿è¡Œæ—¶éªŒè¯

### éœ€æ±‚7: é¡µé¢æ¨¡å—åŒ–ï¼ˆæ–°åŠŸèƒ½ï¼‰
- âœ… ç³»ç»Ÿç›‘æ§é¡µé¢
- âœ… æ§åˆ¶å°ä»ªè¡¨æ¿
- âœ… å¼€å‘è€…å·¥å…·é¡µé¢
- âœ… WebSocketç»ˆç«¯é¡µé¢
- âœ… VLLMä¸“ä¸šç®¡ç†é¡µé¢
- âœ… å“åº”å¼Webç•Œé¢
- âœ… æ¨¡å—åŒ–æ¶æ„è®¾è®¡

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- **Python**: 3.8+
- **æ“ä½œç³»ç»Ÿ**: Linux/Windows/macOS
- **å†…å­˜**: è‡³å°‘2GB RAM
- **ç£ç›˜**: è‡³å°‘1GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: æ”¯æŒSSHè¿æ¥

## ğŸ› ï¸ å®‰è£…éƒ¨ç½²

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®ï¼ˆæˆ–è§£å‹åˆ°aiplatform_pyç›®å½•ï¼‰
cd aiplatform_py

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®ç³»ç»Ÿ

ç¼–è¾‘ `config/config.yaml` æ–‡ä»¶ï¼Œé…ç½®GPUæœåŠ¡å™¨ä¿¡æ¯ï¼š

```yaml
# æœåŠ¡å™¨é…ç½®
server:
  host: "0.0.0.0"
  port: 8088
  workers: 4

# æ•°æ®åº“é…ç½®
database:
  url: "sqlite:///./aiplatform.db"
  echo: false

# GPUæœåŠ¡å™¨é…ç½®
gpu_servers:
  - name: "GPU-Server-1"
    host: "192.168.1.30"  # ä¿®æ”¹ä¸ºå®é™…IP
    port: 22
    username: "your_username"  # ä¿®æ”¹ä¸ºå®é™…ç”¨æˆ·å
    password: "your_password"  # ä¿®æ”¹ä¸ºå®é™…å¯†ç 
    gpu_count: 4
    enabled: true
    model_path: "/path/to/models"  # ä¿®æ”¹ä¸ºå®é™…æ¨¡å‹è·¯å¾„

# ç›‘æ§é…ç½®
monitoring:
  gpu:
    interval: 3  # GPUç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
    history_retention: 24  # å†å²æ•°æ®ä¿ç•™ï¼ˆå°æ—¶ï¼‰
  system:
    interval: 3  # ç³»ç»Ÿç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
    history_retention: 24  # å†å²æ•°æ®ä¿ç•™ï¼ˆå°æ—¶ï¼‰

# VLLMé…ç½®
vllm:
  default_port_range:
    start: 8000
    end: 8100
  default_gpu_memory_utilization: 0.9
  default_max_model_len: 4096
```

### 3. å¯åŠ¨ç³»ç»Ÿ

```bash
# ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
python run.py

# æˆ–ç›´æ¥ä½¿ç”¨uvicorn
uvicorn app.main:app --host 0.0.0.0 --port 8088
```

### 4. è®¿é—®ç³»ç»Ÿ

- **ä¸»é¡µ**: http://localhost:8088
- **ç³»ç»Ÿç›‘æ§**: http://localhost:8088/
- **æ§åˆ¶å°**: http://localhost:8088/dashboard
- **å¼€å‘è€…å·¥å…·**: http://localhost:8088/developer
- **WebSocketç»ˆç«¯**: http://localhost:8088/terminal
- **VLLMç®¡ç†**: http://localhost:8088/vllm
- **APIæ–‡æ¡£**: http://localhost:8088/docs
- **å¥åº·æ£€æŸ¥**: http://localhost:8088/health

## ğŸ“ é¡¹ç›®ç»“æ„

```
aiplatform_py/
â”œâ”€â”€ app/                    # åº”ç”¨æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ api/               # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ gpu.py         # GPUç›‘æ§API
â”‚   â”‚   â”œâ”€â”€ models.py      # æ¨¡å‹ç®¡ç†API
â”‚   â”‚   â”œâ”€â”€ system.py      # ç³»ç»Ÿç›‘æ§API
â”‚   â”‚   â”œâ”€â”€ ssh.py         # SSHç®¡ç†API
â”‚   â”‚   â”œâ”€â”€ config.py      # é…ç½®ç®¡ç†API
â”‚   â”‚   â””â”€â”€ vllm_management.py # VLLMç®¡ç†APIï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ models/            # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ base.py        # åŸºç¡€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ gpu_resource.py    # GPUèµ„æºæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ model_service.py   # æ¨¡å‹æœåŠ¡æ¨¡å‹
â”‚   â”‚   â””â”€â”€ system_resource.py # ç³»ç»Ÿèµ„æºæ¨¡å‹
â”‚   â”œâ”€â”€ pages/             # é¡µé¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ dashboard.py       # æ§åˆ¶å°é¡µé¢
â”‚   â”‚   â”œâ”€â”€ system_monitor.py  # ç³»ç»Ÿç›‘æ§é¡µé¢
â”‚   â”‚   â”œâ”€â”€ developer_tools.py # å¼€å‘è€…å·¥å…·é¡µé¢
â”‚   â”‚   â”œâ”€â”€ terminal.py        # ç»ˆç«¯é¡µé¢
â”‚   â”‚   â””â”€â”€ vllm_management_page.py # VLLMç®¡ç†é¡µé¢ï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ services/          # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ ssh_manager.py     # SSHè¿æ¥ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ gpu_monitor.py     # GPUç›‘æ§æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ system_monitor.py  # ç³»ç»Ÿç›‘æ§æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ model_service.py   # æ¨¡å‹æœåŠ¡ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ websocket_terminal.py # WebSocketç»ˆç«¯æœåŠ¡
â”‚   â”‚   â””â”€â”€ enhanced_diagnosis.py # å¢å¼ºè¯Šæ–­æœåŠ¡ï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ static/            # é™æ€èµ„æº
â”‚   â”‚   â”œâ”€â”€ css/           # æ ·å¼æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ js/            # JavaScriptæ–‡ä»¶
â”‚   â”‚   â””â”€â”€ templates/     # æ¨¡æ¿æ–‡ä»¶
â”‚   â”œâ”€â”€ config.py          # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py        # æ•°æ®åº“ç®¡ç†
â”‚   â””â”€â”€ main.py           # åº”ç”¨å…¥å£
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ config.yaml       # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ debug_test/           # è°ƒè¯•æµ‹è¯•
â”‚   â”œâ”€â”€ check_server_status.py  # æœåŠ¡å™¨çŠ¶æ€æ£€æŸ¥
â”‚   â”œâ”€â”€ cleanup_invalid_records.py # æ¸…ç†æ— æ•ˆè®°å½•
â”‚   â”œâ”€â”€ test_api.py       # APIæµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ check_db.py       # æ•°æ®åº“æ£€æŸ¥
â”‚   â”œâ”€â”€ debug_gpu.py      # GPUè°ƒè¯•è„šæœ¬
â”‚   â”œâ”€â”€ gpu_diagnosis_enhanced.py # å¢å¼ºGPUè¯Šæ–­
â”‚   â”œâ”€â”€ test_vllm_management.py # VLLMç®¡ç†æµ‹è¯•
â”‚   â””â”€â”€ quick_diagnosis.py # å¿«é€Ÿè¯Šæ–­å·¥å…·
â”œâ”€â”€ docs/                 # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ VLLM_COMPLETE_FEATURES.md # VLLMåŠŸèƒ½æ¸…å•
â”‚   â”œâ”€â”€ VLLM_SEPARATION_SUMMARY.md # VLLMåˆ†ç¦»æ€»ç»“
â”‚   â”œâ”€â”€ DIAGNOSIS_FUNCTION_FIX.md # è¯Šæ–­åŠŸèƒ½ä¿®å¤
â”‚   â””â”€â”€ LOADING_STATE_FIX.md # åŠ è½½çŠ¶æ€ä¿®å¤
â”œâ”€â”€ logs/                 # æ—¥å¿—ç›®å½•
â”œâ”€â”€ data/                 # æ•°æ®ç›®å½•
â”œâ”€â”€ models/               # æ¨¡å‹å­˜å‚¨ç›®å½•
â”œâ”€â”€ requirements.txt      # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ run.py               # å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md            # é¡¹ç›®æ–‡æ¡£
```

## ğŸ”§ APIæ¥å£

### GPUç›‘æ§

```bash
# è·å–å½“å‰GPUçŠ¶æ€
GET /api/gpu/current

# è·å–æŒ‡å®šæœåŠ¡å™¨GPUçŠ¶æ€
GET /api/gpu/servers/{server_name}

# è·å–GPUå†å²æ•°æ®
GET /api/gpu/history/{server_name}/{gpu_index}?hours=1

# è·å–GPUç»Ÿè®¡æ‘˜è¦
GET /api/gpu/summary

# WebSocket GPUæ•°æ®æ¨é€
WS /ws/gpu/{server_name}
```

### æ¨¡å‹ç®¡ç†

```bash
# è·å–æ‰€æœ‰æ¨¡å‹
GET /api/models/

# å¯åŠ¨æ¨¡å‹æœåŠ¡
POST /api/models/{model_id}/start

# åœæ­¢æ¨¡å‹æœåŠ¡
POST /api/models/{model_id}/stop

# å‘ç°æœåŠ¡å™¨æ¨¡å‹
GET /api/models/discover/{server_name}

# æ·»åŠ æ¨¡å‹
POST /api/models/
Content-Type: application/json
{
    "name": "chatglm3-6b",
    "model_path": "/home/models/chatglm3-6b",
    "server_name": "GPU-Server-1",
    "gpu_indices": "0",
    "max_model_len": 4096
}
```

### ç³»ç»Ÿç›‘æ§

```bash
# è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€
GET /api/system/current

# è·å–æŒ‡å®šæœåŠ¡å™¨ç³»ç»Ÿèµ„æº
GET /api/system/servers/{server_name}

# è·å–ç³»ç»Ÿå†å²æ•°æ®
GET /api/system/history/{server_name}?hours=1

# WebSocketç³»ç»Ÿæ•°æ®æ¨é€
WS /ws/system/{server_name}
```

### SSHç®¡ç†

```bash
# æ‰§è¡ŒSSHå‘½ä»¤
POST /api/ssh/servers/{server_name}/execute
Content-Type: application/json
{
    "command": "nvidia-smi",
    "timeout": 30
}

# æ£€æŸ¥SSHè¿æ¥çŠ¶æ€
GET /api/ssh/servers/{server_name}/status

# è·å–æœåŠ¡å™¨ç³»ç»Ÿä¿¡æ¯
GET /api/ssh/servers/{server_name}/system-info

# WebSocket SSHç»ˆç«¯
WS /ws/terminal/{server_name}
```

### é…ç½®ç®¡ç†

```bash
# æ·»åŠ GPUæœåŠ¡å™¨
POST /api/config/servers
Content-Type: application/json
{
    "name": "GPU-Server-3",
    "host": "192.168.1.32",
    "username": "admin",
    "password": "password",
    "gpu_count": 8,
    "model_path": "/home/admin/models"
}

# æµ‹è¯•æœåŠ¡å™¨è¿æ¥
POST /api/config/servers/{server_name}/test

# æ›´æ–°æœåŠ¡å™¨é…ç½®
PUT /api/config/servers/{server_name}

# åˆ é™¤æœåŠ¡å™¨é…ç½®
DELETE /api/config/servers/{server_name}
```

### VLLMç®¡ç†ï¼ˆæ–°åŠŸèƒ½ï¼‰

```bash
# è·å–æœåŠ¡å™¨åˆ—è¡¨
GET /api/vllm/servers

# ç¯å¢ƒè¯Šæ–­
GET /api/vllm/diagnose/{server_name}

# å‘ç°æ¨¡å‹
GET /api/vllm/models/{server_name}

# è·å–è¿è¡Œä¸­æœåŠ¡
GET /api/vllm/running/{server_name}

# è·å–æœåŠ¡æ—¥å¿—
GET /api/vllm/logs/{server_name}/{port}?lines=100

# æ£€æŸ¥ç«¯å£ä½¿ç”¨
GET /api/vllm/ports/{server_name}

# å¯åŠ¨VLLMæœåŠ¡
POST /api/vllm/start
Content-Type: application/json
{
    "server_name": "GPU-Server-1",
    "model_path": "/path/to/model",
    "port": 8000,
    "gpu_indices": "0",
    "tensor_parallel_size": 1,
    "max_model_len": 4096,
    "gpu_memory_utilization": 0.9,
    "dtype": "auto",
    "quantization": null,
    "trust_remote_code": false
}

# åœæ­¢VLLMæœåŠ¡
POST /api/vllm/stop
Content-Type: application/json
{
    "server_name": "GPU-Server-1",
    "port": 8000
}

# é‡å¯VLLMæœåŠ¡
POST /api/vllm/restart

# è·å–æœåŠ¡çŠ¶æ€
GET /api/vllm/status/{server_name}

# å¥åº·æ£€æŸ¥
GET /api/vllm/health/{server_name}/{port}

# æ€§èƒ½ç›‘æ§
GET /api/vllm/performance/{server_name}

# æ‰¹é‡æ“ä½œ
POST /api/vllm/batch-operation

# é…ç½®ç®¡ç†
GET /api/vllm/saved-models
POST /api/vllm/save-model
DELETE /api/vllm/models/{model_id}
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### 1. ç›‘æ§GPUçŠ¶æ€

```python
import requests

# è·å–æ‰€æœ‰GPUçŠ¶æ€
response = requests.get("http://localhost:8088/api/gpu/current")
gpu_data = response.json()

for gpu in gpu_data["data"]:
    print(f"æœåŠ¡å™¨: {gpu['server_name']}")
    print(f"GPU {gpu['gpu_index']}: {gpu['utilization_gpu']}% ä½¿ç”¨ç‡")
```

### 2. å¯åŠ¨æ¨¡å‹æœåŠ¡

```python
import requests

# æ·»åŠ æ¨¡å‹
model_data = {
    "name": "chatglm3-6b",
    "model_path": "/home/models/chatglm3-6b",
    "server_name": "GPU-Server-1",
    "gpu_indices": "0",
    "max_model_len": 4096
}

response = requests.post("http://localhost:8088/api/models/", json=model_data)
model = response.json()["data"]

# å¯åŠ¨æ¨¡å‹æœåŠ¡
model_id = model["id"]
response = requests.post(f"http://localhost:8088/api/models/{model_id}/start")
print(response.json()["message"])
```

### 3. ä½¿ç”¨WebSocketç»ˆç«¯

```javascript
// è¿æ¥WebSocketç»ˆç«¯
const ws = new WebSocket('ws://localhost:8088/ws/terminal/GPU-Server-1');

ws.onopen = function() {
    console.log('ç»ˆç«¯è¿æ¥å·²å»ºç«‹');
};

ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    if (data.type === 'output') {
        console.log('ç»ˆç«¯è¾“å‡º:', data.message);
    }
};

// å‘é€å‘½ä»¤
ws.send(JSON.stringify({
    type: 'input',
    data: 'ls -la\n'
}));
```

### 4. æ‰§è¡Œè¿œç¨‹å‘½ä»¤

```python
import requests

# æ‰§è¡ŒGPUæŸ¥è¯¢å‘½ä»¤
command_data = {
    "command": "nvidia-smi --query-gpu=name,temperature.gpu --format=csv",
    "timeout": 10
}

response = requests.post(
    "http://localhost:8088/api/ssh/servers/GPU-Server-1/execute",
    json=command_data
)

result = response.json()["data"]
print(f"å‘½ä»¤è¾“å‡º: {result['stdout']}")
```

### 5. VLLMæœåŠ¡ç®¡ç†

```python
import requests

# ç¯å¢ƒè¯Šæ–­
response = requests.get("http://localhost:8088/api/vllm/diagnose/GPU-Server-1")
diagnosis = response.json()["data"]
print(f"GPUå¯ç”¨: {diagnosis['gpu_available']}")

# å‘ç°æ¨¡å‹
response = requests.get("http://localhost:8088/api/vllm/models/GPU-Server-1")
models = response.json()["data"]["discovered_models"]
print(f"å‘ç° {len(models)} ä¸ªæ¨¡å‹")

# å¯åŠ¨VLLMæœåŠ¡
vllm_config = {
    "server_name": "GPU-Server-1",
    "model_path": "/path/to/chatglm3-6b",
    "port": 8000,
    "gpu_indices": "0",
    "tensor_parallel_size": 1,
    "max_model_len": 4096,
    "gpu_memory_utilization": 0.9,
    "dtype": "auto"
}

response = requests.post(
    "http://localhost:8088/api/vllm/start",
    json=vllm_config
)
print(response.json()["message"])

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
response = requests.get("http://localhost:8088/api/vllm/status/GPU-Server-1")
status = response.json()["data"]
print(f"è¿è¡Œä¸­æœåŠ¡: {len(status['running_services'])}")

# å¥åº·æ£€æŸ¥
response = requests.get("http://localhost:8088/api/vllm/health/GPU-Server-1/8000")
health = response.json()["data"]
print(f"æœåŠ¡å¥åº·çŠ¶æ€: {health['status']}")
```

### 6. é…ç½®æ¨¡æ¿ç®¡ç†

```python
import requests

# ä¿å­˜é…ç½®æ¨¡æ¿
config_template = {
    "name": "ChatGLM3-6B-Template",
    "description": "ChatGLM3-6Bä¼˜åŒ–é…ç½®",
    "config": {
        "model_path": "/path/to/chatglm3-6b",
        "tensor_parallel_size": 1,
        "max_model_len": 8192,
        "gpu_memory_utilization": 0.85,
        "dtype": "half"
    }
}

response = requests.post(
    "http://localhost:8088/api/vllm/save-model",
    json=config_template
)
print("é…ç½®æ¨¡æ¿å·²ä¿å­˜")

# è·å–å·²ä¿å­˜çš„é…ç½®
response = requests.get("http://localhost:8088/api/vllm/saved-models")
templates = response.json()["data"]
print(f"å·²ä¿å­˜ {len(templates)} ä¸ªé…ç½®æ¨¡æ¿")
```

## ğŸŒ Webç•Œé¢åŠŸèƒ½

### ç³»ç»Ÿç›‘æ§é¡µé¢
- å®æ—¶GPUä½¿ç”¨ç‡å›¾è¡¨
- ç³»ç»Ÿèµ„æºç›‘æ§é¢æ¿
- æœåŠ¡å™¨çŠ¶æ€æ¦‚è§ˆ
- å†å²æ•°æ®è¶‹åŠ¿å›¾

### æ§åˆ¶å°ä»ªè¡¨æ¿
- ç»¼åˆæ•°æ®å±•ç¤º
- å¿«é€Ÿæ“ä½œé¢æ¿
- æ¨¡å‹æœåŠ¡ç®¡ç†
- ç³»ç»ŸçŠ¶æ€æ‘˜è¦

### å¼€å‘è€…å·¥å…·é¡µé¢
- APIæ–‡æ¡£é“¾æ¥
- åŸå§‹æ•°æ®è®¿é—®
- ç³»ç»Ÿè¯Šæ–­å·¥å…·
- æ•…éšœæ’æŸ¥æŒ‡å—
- å¢å¼ºçš„GPUç¯å¢ƒè¯Šæ–­
- Pythonç¯å¢ƒæ£€æµ‹

### WebSocketç»ˆç«¯é¡µé¢
- å®æ—¶SSHç»ˆç«¯
- å¤šæœåŠ¡å™¨ä¼šè¯ç®¡ç†
- äº¤äº’å¼å‘½ä»¤æ‰§è¡Œ
- ç»ˆç«¯å†å²è®°å½•

### VLLMç®¡ç†é¡µé¢ï¼ˆæ–°åŠŸèƒ½ï¼‰
- ä¸“ä¸šçš„VLLMæœåŠ¡ç®¡ç†ç•Œé¢
- å®Œæ•´çš„å¯åŠ¨å‚æ•°é…ç½®
- æ™ºèƒ½å‚æ•°é¢„è®¾ï¼ˆå°/ä¸­/å¤§æ¨¡å‹ï¼‰
- é…ç½®æ¨¡æ¿ä¿å­˜å’ŒåŠ è½½
- å®æ—¶æ€§èƒ½ç›‘æ§é¢æ¿
- æœåŠ¡å¥åº·æ£€æŸ¥
- ç¯å¢ƒè¯Šæ–­å·¥å…·
- æ¨¡å‹å‘ç°å’Œç®¡ç†
- æœåŠ¡æ—¥å¿—æŸ¥çœ‹
- æ‰¹é‡æ“ä½œæ”¯æŒ

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **SSHè¿æ¥å¤±è´¥**
   - æ£€æŸ¥æœåŠ¡å™¨IPã€ç«¯å£ã€ç”¨æˆ·åå¯†ç æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æœåŠ¡å™¨SSHæœåŠ¡å·²å¯åŠ¨
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

2. **GPUç›‘æ§æ— æ•°æ®**
   - ç¡®è®¤æœåŠ¡å™¨å·²å®‰è£…nvidia-smi
   - æ£€æŸ¥GPUé©±åŠ¨æ˜¯å¦æ­£å¸¸
   - éªŒè¯SSHè¿æ¥æ˜¯å¦æ­£å¸¸

3. **æ¨¡å‹å¯åŠ¨å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
   - ç¡®è®¤vllmç¯å¢ƒå·²å®‰è£…
   - æŸ¥çœ‹ç«¯å£æ˜¯å¦è¢«å ç”¨

4. **WebSocketè¿æ¥å¤±è´¥**
   - æ£€æŸ¥æµè§ˆå™¨WebSocketæ”¯æŒ
   - ç¡®è®¤æœåŠ¡å™¨ç«¯å£é…ç½®æ­£ç¡®
   - æŸ¥çœ‹ç½‘ç»œè¿æ¥çŠ¶æ€

5. **æ•°æ®åº“é”™è¯¯**
   - æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
   - ç¡®è®¤SQLiteæ”¯æŒ
   - æ¸…ç†æ—§çš„æ•°æ®åº“æ–‡ä»¶

6. **VLLMç¯å¢ƒé—®é¢˜**
   - ä½¿ç”¨ç¯å¢ƒè¯Šæ–­åŠŸèƒ½æ£€æŸ¥GPUå¯ç”¨æ€§
   - ç¡®è®¤PyTorch CUDAæ”¯æŒæ­£å¸¸
   - æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–åŒ…
   - éªŒè¯CUDAè¿è¡Œæ—¶åº“å®‰è£…

7. **VLLMæœåŠ¡å¯åŠ¨å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™
   - ç¡®è®¤ç«¯å£æœªè¢«å ç”¨
   - éªŒè¯GPUå†…å­˜æ˜¯å¦å……è¶³
   - æŸ¥çœ‹æœåŠ¡æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

8. **é…ç½®å‚æ•°é”™è¯¯**
   - ä½¿ç”¨æ™ºèƒ½é¢„è®¾é¿å…å‚æ•°å†²çª
   - æ£€æŸ¥å¼ é‡å¹¶è¡Œåº¦è®¾ç½®
   - éªŒè¯GPUå†…å­˜åˆ©ç”¨ç‡é…ç½®
   - ç¡®è®¤æ•°æ®ç±»å‹å’Œé‡åŒ–è®¾ç½®å…¼å®¹æ€§

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
tail -f aiplatform.log

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
python run.py --log-level debug

# è¿è¡ŒAPIæµ‹è¯•
python debug_test/test_api.py

# VLLMç®¡ç†æµ‹è¯•
python debug_test/test_vllm_management.py

# å¢å¼ºGPUè¯Šæ–­
python debug_test/gpu_diagnosis_enhanced.py

# å¿«é€Ÿè¯Šæ–­å·¥å…·
python debug_test/quick_diagnosis.py

# æ£€æŸ¥æ¨¡å‹æœåŠ¡çŠ¶æ€
python debug_test/check_model_services.py
```

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é¡¹

1. **å¯†ç å®‰å…¨**: é…ç½®æ–‡ä»¶ä¸­çš„å¯†ç ä»¥æ˜æ–‡å­˜å‚¨ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æƒé™è®¾ç½®æ­£ç¡®
2. **ç½‘ç»œå®‰å…¨**: ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨HTTPSå’Œé˜²ç«å¢™
3. **SSHå¯†é’¥**: æ¨èä½¿ç”¨SSHå¯†é’¥è€Œéå¯†ç è®¤è¯
4. **è®¿é—®æ§åˆ¶**: è€ƒè™‘æ·»åŠ èº«ä»½éªŒè¯å’Œæˆæƒæœºåˆ¶
5. **WebSocketå®‰å…¨**: ç”Ÿäº§ç¯å¢ƒå»ºè®®æ·»åŠ WebSocketè®¤è¯

## ğŸ†• æœ€æ–°æ›´æ–°

### v1.2.0 (å½“å‰ç‰ˆæœ¬)
- âœ¨ æ–°å¢ç‹¬ç«‹VLLMç®¡ç†é¡µé¢
- âœ¨ å®Œæ•´çš„VLLMå‚æ•°é…ç½®æ”¯æŒ
- âœ¨ æ™ºèƒ½å‚æ•°é¢„è®¾å’Œé…ç½®æ¨¡æ¿
- âœ¨ å¢å¼ºçš„GPUç¯å¢ƒè¯Šæ–­åŠŸèƒ½
- âœ¨ å®æ—¶æ€§èƒ½ç›‘æ§å’Œå¥åº·æ£€æŸ¥
- âœ¨ é…ç½®ä¿å­˜å’Œæ‰¹é‡æ“ä½œ
- âœ¨ æ¨¡å—åŒ–æ¶æ„ä¼˜åŒ–
- âœ¨ ä¼ä¸šçº§åŠŸèƒ½å®Œå–„

### v1.1.0
- âœ¨ æ–°å¢WebSocketå®æ—¶ç»ˆç«¯åŠŸèƒ½
- âœ¨ æ–°å¢å¼€å‘è€…å·¥å…·é¡µé¢
- âœ¨ é¡µé¢æ¨¡å—åŒ–é‡æ„
- âœ¨ å®æ—¶æ•°æ®æ¨é€åŠŸèƒ½
- âœ¨ æ”¹è¿›çš„é…ç½®ç®¡ç†ç³»ç»Ÿ
- âœ¨ å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

### v1.0.0
- ğŸ‰ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… GPUæœåŠ¡å™¨ç›‘æ§
- âœ… æ¨¡å‹æœåŠ¡ç®¡ç†
- âœ… SSHè¿œç¨‹æ§åˆ¶
- âœ… åŸºç¡€Webç•Œé¢

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd aiplatform_py

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt

# è¿è¡ŒAPIæµ‹è¯•
python debug_test/test_api.py

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
python run.py
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- åˆ›å»ºGitHub Issue
- å‘é€é‚®ä»¶åˆ°é¡¹ç›®ç»´æŠ¤è€…
- æŸ¥çœ‹é¡¹ç›®Wikiæ–‡æ¡£

---

**æ„Ÿè°¢ä½¿ç”¨AIå¹³å°ç®¡ç†ç³»ç»Ÿï¼** ğŸ‰