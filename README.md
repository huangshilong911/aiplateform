# AIå¹³å°ç®¡ç†ç³»ç»Ÿ (Pythonç‰ˆ)

åŸºäºFastAPIæ„å»ºçš„GPUæœåŠ¡å™¨ç›‘æ§å’Œå¤§æ¨¡å‹æœåŠ¡ç®¡ç†å¹³å°ï¼Œæä¾›å®æ—¶ç›‘æ§ã€æ¨¡å‹ç®¡ç†ã€SSHè¿œç¨‹æ§åˆ¶ç­‰åŠŸèƒ½ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### éœ€æ±‚1: GPUæœåŠ¡å™¨ç›‘æ§
- âœ… å®æ—¶ç›‘æ§GPUä½¿ç”¨ç‡ã€å†…å­˜ã€æ¸©åº¦ã€åŠŸè€—ç­‰æŒ‡æ ‡
- âœ… æ”¯æŒå¤šæœåŠ¡å™¨åŒæ—¶ç›‘æ§
- âœ… å†å²æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢
- âœ… GPUçŠ¶æ€åˆ†æå’Œç»Ÿè®¡

### éœ€æ±‚2: æ¨¡å‹åˆ—è¡¨ç®¡ç†
- âœ… è‡ªåŠ¨å‘ç°æœåŠ¡å™¨ä¸Šçš„å¤§æ¨¡å‹æ–‡ä»¶
- âœ… æ¨¡å‹ä¿¡æ¯å±•ç¤ºï¼ˆè·¯å¾„ã€å¤§å°ã€ç±»å‹ç­‰ï¼‰
- âœ… æ¨¡å‹åˆ†ç±»å’Œç»„ç»‡ç®¡ç†
- âœ… æ‰¹é‡æ¨¡å‹æ“ä½œ

### éœ€æ±‚3: æ¨¡å‹æœåŠ¡æ§åˆ¶
- âœ… åœ¨çº¿å¯åŠ¨/åœæ­¢å¤§æ¨¡å‹æœåŠ¡
- âœ… æ”¯æŒVLLMæ¡†æ¶
- âœ… æœåŠ¡çŠ¶æ€ç›‘æ§
- âœ… ç«¯å£ç®¡ç†å’Œèµ„æºåˆ†é…
- âœ… æœåŠ¡æ—¥å¿—æŸ¥çœ‹

### éœ€æ±‚4: SSHè¿œç¨‹ç™»å½•
- âœ… å®‰å…¨çš„SSHè¿æ¥ç®¡ç†
- âœ… è¿œç¨‹å‘½ä»¤æ‰§è¡Œ
- âœ… è¿æ¥çŠ¶æ€ç›‘æ§
- âœ… å¤šæœåŠ¡å™¨ä¼šè¯ç®¡ç†

### éœ€æ±‚5: é…ç½®ç®¡ç†
- âœ… åŠ¨æ€æ·»åŠ /ä¿®æ”¹/åˆ é™¤GPUæœåŠ¡å™¨
- âœ… è¿æ¥é…ç½®æµ‹è¯•
- âœ… ç³»ç»Ÿå‚æ•°é…ç½®
- âœ… é…ç½®æ–‡ä»¶ç®¡ç†

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- **Python**: 3.8+
- **æ“ä½œç³»ç»Ÿ**: Linux/Windows/macOS
- **å†…å­˜**: è‡³å°‘2GB RAM
- **ç£ç›˜**: è‡³å°‘1GBå¯ç”¨ç©ºé—´

## ğŸ› ï¸ å®‰è£…éƒ¨ç½²

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®ï¼ˆæˆ–è§£å‹åˆ°aiplatform_pyç›®å½•ï¼‰
cd aiplatform_py

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®ç³»ç»Ÿ

ç¼–è¾‘ `config/config.yaml` æ–‡ä»¶ï¼Œé…ç½®GPUæœåŠ¡å™¨ä¿¡æ¯ï¼š

```yaml
# GPUæœåŠ¡å™¨é…ç½®
gpu_servers:
  - name: "GPU-Server-1"
    host: "192.168.1.30"  # ä¿®æ”¹ä¸ºå®é™…IP
    port: 22
    username: "your_username"  # ä¿®æ”¹ä¸ºå®é™…ç”¨æˆ·å
    password: "your_password"  # ä¿®æ”¹ä¸ºå®é™…å¯†ç 
    gpu_count: 4
    enabled: true
    model_path: "/path/to/models"  # ä¿®æ”¹ä¸ºå®é™…æ¨¡å‹è·¯å¾„
```

### 3. å¯åŠ¨ç³»ç»Ÿ

```bash
# ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
python run.py

# æˆ–ç›´æ¥ä½¿ç”¨uvicorn
uvicorn app.main:app --host 0.0.0.0 --port 8088
```

### 4. è®¿é—®ç³»ç»Ÿ

- **ä¸»é¡µ**: http://localhost:8088
- **APIæ–‡æ¡£**: http://localhost:8088/docs
- **å¥åº·æ£€æŸ¥**: http://localhost:8088/health

## ğŸ“ é¡¹ç›®ç»“æ„

```
aiplatform_py/
â”œâ”€â”€ app/                    # åº”ç”¨æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ api/               # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ gpu.py         # GPUç›‘æ§API
â”‚   â”‚   â”œâ”€â”€ models.py      # æ¨¡å‹ç®¡ç†API
â”‚   â”‚   â”œâ”€â”€ system.py      # ç³»ç»Ÿç›‘æ§API
â”‚   â”‚   â”œâ”€â”€ ssh.py         # SSHç®¡ç†API
â”‚   â”‚   â””â”€â”€ config.py      # é…ç½®ç®¡ç†API
â”‚   â”œâ”€â”€ models/            # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ base.py        # åŸºç¡€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ gpu_resource.py    # GPUèµ„æºæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ model_service.py   # æ¨¡å‹æœåŠ¡æ¨¡å‹
â”‚   â”‚   â””â”€â”€ system_resource.py # ç³»ç»Ÿèµ„æºæ¨¡å‹
â”‚   â”œâ”€â”€ services/          # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ ssh_manager.py     # SSHè¿æ¥ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ gpu_monitor.py     # GPUç›‘æ§æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ system_monitor.py  # ç³»ç»Ÿç›‘æ§æœåŠ¡
â”‚   â”‚   â””â”€â”€ model_service.py   # æ¨¡å‹æœåŠ¡ç®¡ç†
â”‚   â”œâ”€â”€ config.py          # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ database.py        # æ•°æ®åº“ç®¡ç†
â”‚   â””â”€â”€ main.py           # åº”ç”¨å…¥å£
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ config.yaml       # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt      # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ run.py               # å¯åŠ¨è„šæœ¬
â””â”€â”€ README.md            # é¡¹ç›®æ–‡æ¡£
```

## ğŸ”§ APIæ¥å£

### GPUç›‘æ§

```bash
# è·å–å½“å‰GPUçŠ¶æ€
GET /api/gpu/current

# è·å–æŒ‡å®šæœåŠ¡å™¨GPUçŠ¶æ€
GET /api/gpu/servers/{server_name}

# è·å–GPUå†å²æ•°æ®
GET /api/gpu/history/{server_name}/{gpu_index}?hours=1

# è·å–GPUç»Ÿè®¡æ‘˜è¦
GET /api/gpu/summary
```

### æ¨¡å‹ç®¡ç†

```bash
# è·å–æ‰€æœ‰æ¨¡å‹
GET /api/models/

# å¯åŠ¨æ¨¡å‹æœåŠ¡
POST /api/models/{model_id}/start

# åœæ­¢æ¨¡å‹æœåŠ¡
POST /api/models/{model_id}/stop

# å‘ç°æœåŠ¡å™¨æ¨¡å‹
GET /api/models/discover/{server_name}
```

### ç³»ç»Ÿç›‘æ§

```bash
# è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€
GET /api/system/current

# è·å–æŒ‡å®šæœåŠ¡å™¨ç³»ç»Ÿèµ„æº
GET /api/system/servers/{server_name}

# è·å–ç³»ç»Ÿå†å²æ•°æ®
GET /api/system/history/{server_name}?hours=1
```

### SSHç®¡ç†

```bash
# æ‰§è¡ŒSSHå‘½ä»¤
POST /api/ssh/servers/{server_name}/execute
Content-Type: application/json
{
    "command": "nvidia-smi",
    "timeout": 30
}

# æ£€æŸ¥SSHè¿æ¥çŠ¶æ€
GET /api/ssh/servers/{server_name}/status

# è·å–æœåŠ¡å™¨ç³»ç»Ÿä¿¡æ¯
GET /api/ssh/servers/{server_name}/system-info
```

### é…ç½®ç®¡ç†

```bash
# æ·»åŠ GPUæœåŠ¡å™¨
POST /api/config/servers
Content-Type: application/json
{
    "name": "GPU-Server-3",
    "host": "192.168.1.32",
    "username": "admin",
    "password": "password",
    "gpu_count": 8,
    "model_path": "/home/admin/models"
}

# æµ‹è¯•æœåŠ¡å™¨è¿æ¥
POST /api/config/servers/{server_name}/test
```

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### 1. ç›‘æ§GPUçŠ¶æ€

```python
import requests

# è·å–æ‰€æœ‰GPUçŠ¶æ€
response = requests.get("http://localhost:8088/api/gpu/current")
gpu_data = response.json()

for gpu in gpu_data["data"]:
    print(f"æœåŠ¡å™¨: {gpu['server_name']}")
    print(f"GPU {gpu['gpu_index']}: {gpu['utilization_gpu']}% ä½¿ç”¨ç‡")
```

### 2. å¯åŠ¨æ¨¡å‹æœåŠ¡

```python
import requests

# æ·»åŠ æ¨¡å‹
model_data = {
    "name": "chatglm3-6b",
    "model_path": "/home/models/chatglm3-6b",
    "server_name": "GPU-Server-1",
    "gpu_indices": "0",
    "max_model_len": 4096
}

response = requests.post("http://localhost:8088/api/models/", json=model_data)
model = response.json()["data"]

# å¯åŠ¨æ¨¡å‹æœåŠ¡
model_id = model["id"]
response = requests.post(f"http://localhost:8088/api/models/{model_id}/start")
print(response.json()["message"])
```

### 3. æ‰§è¡Œè¿œç¨‹å‘½ä»¤

```python
import requests

# æ‰§è¡ŒGPUæŸ¥è¯¢å‘½ä»¤
command_data = {
    "command": "nvidia-smi --query-gpu=name,temperature.gpu --format=csv",
    "timeout": 10
}

response = requests.post(
    "http://localhost:8088/api/ssh/servers/GPU-Server-1/execute",
    json=command_data
)

result = response.json()["data"]
print(f"å‘½ä»¤è¾“å‡º: {result['stdout']}")
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **SSHè¿æ¥å¤±è´¥**
   - æ£€æŸ¥æœåŠ¡å™¨IPã€ç«¯å£ã€ç”¨æˆ·åå¯†ç æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æœåŠ¡å™¨SSHæœåŠ¡å·²å¯åŠ¨
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

2. **GPUç›‘æ§æ— æ•°æ®**
   - ç¡®è®¤æœåŠ¡å™¨å·²å®‰è£…nvidia-smi
   - æ£€æŸ¥GPUé©±åŠ¨æ˜¯å¦æ­£å¸¸
   - éªŒè¯SSHè¿æ¥æ˜¯å¦æ­£å¸¸

3. **æ¨¡å‹å¯åŠ¨å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
   - ç¡®è®¤vllmç¯å¢ƒå·²å®‰è£…
   - æŸ¥çœ‹ç«¯å£æ˜¯å¦è¢«å ç”¨

4. **æ•°æ®åº“é”™è¯¯**
   - æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æƒé™
   - ç¡®è®¤SQLiteæ”¯æŒ
   - æ¸…ç†æ—§çš„æ•°æ®åº“æ–‡ä»¶

### æ—¥å¿—æŸ¥çœ‹

```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
tail -f aiplatform.log

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
python run.py --log-level debug
```

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é¡¹

1. **å¯†ç å®‰å…¨**: é…ç½®æ–‡ä»¶ä¸­çš„å¯†ç ä»¥æ˜æ–‡å­˜å‚¨ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æƒé™è®¾ç½®æ­£ç¡®
2. **ç½‘ç»œå®‰å…¨**: ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨HTTPSå’Œé˜²ç«å¢™
3. **SSHå¯†é’¥**: æ¨èä½¿ç”¨SSHå¯†é’¥è€Œéå¯†ç è®¤è¯
4. **è®¿é—®æ§åˆ¶**: è€ƒè™‘æ·»åŠ èº«ä»½éªŒè¯å’Œæˆæƒæœºåˆ¶

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- åˆ›å»ºGitHub Issue
- å‘é€é‚®ä»¶åˆ°é¡¹ç›®ç»´æŠ¤è€…

---

**æ„Ÿè°¢ä½¿ç”¨AIå¹³å°ç®¡ç†ç³»ç»Ÿï¼** ğŸ‰ 