(vllm) root@ubuntu:/home/hsl# python -m vllm.entrypoints.openai.api_server --model /home/hsl/workspace_hsl/model/Qwen3-14B --port 8000 --max-model-len 4096 --gpu-memory-utilization 0.9 --tensor-parallel-size 2 --host 0.0.0.0 --trust-remote-code --dtype auto --quantization None 
INFO 08-06 11:04:25 [__init__.py:235] Automatically detected platform cuda.
INFO 08-06 11:04:26 [api_server.py:1755] vLLM API server version 0.10.0
INFO 08-06 11:04:26 [cli_args.py:261] non-default args: {'host': '0.0.0.0', 'model': '/home/hsl/workspace_hsl/model/Qwen3-14B', 'trust_remote_code': True, 'max_model_len': 4096, 'tensor_parallel_size': 2}
INFO 08-06 11:04:31 [config.py:1604] Using max model len 4096
INFO 08-06 11:04:32 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 08-06 11:04:36 [__init__.py:235] Automatically detected platform cuda.
INFO 08-06 11:04:37 [core.py:572] Waiting for init message from front-end.
INFO 08-06 11:04:37 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='/home/hsl/workspace_hsl/model/Qwen3-14B', speculative_config=None, tokenizer='/home/hsl/workspace_hsl/model/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/hsl/workspace_hsl/model/Qwen3-14B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 08-06 11:04:37 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 08-06 11:04:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_9f3b903c'), local_subscribe_addr='ipc:///tmp/03a4b282-5934-4e67-9c75-fa92ba855951', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 08-06 11:04:40 [__init__.py:235] Automatically detected platform cuda.
INFO 08-06 11:04:40 [__init__.py:235] Automatically detected platform cuda.
(VllmWorker rank=1 pid=3374856) INFO 08-06 11:04:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_06040958'), local_subscribe_addr='ipc:///tmp/c90a1410-e967-48e1-bb8f-49c5626efb71', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=0 pid=3374855) INFO 08-06 11:04:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3a63a052'), local_subscribe_addr='ipc:///tmp/c4cd7df5-a84e-4874-96d1-de32418b6d1e', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511] WorkerProc failed to start.
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511] Traceback (most recent call last):
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 485, in worker_main
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]     worker = WorkerProc(*args, **kwargs)
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 381, in __init__
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]     self.worker.init_device()
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/worker/worker_base.py", line 603, in init_device
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]     self.worker.init_device()  # type: ignore
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 168, in init_device
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511]     raise ValueError(
(VllmWorker rank=0 pid=3374855) ERROR 08-06 11:04:43 [multiproc_executor.py:511] ValueError: Free memory on device (38.01/44.52 GiB) on startup is less than desired GPU memory utilization (0.9, 40.07 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[W806 11:04:43.290695448 TCPStore.cpp:343] [c10d] TCP client failed to connect/validate to host 127.0.0.1:44587 - retrying (try=0, timeout=600000ms, delay=712ms): Interrupted system call
Exception raised from delay at /pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7c7b26b785e8 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8bfe (0x7c7b0ffa8bfe in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x1369d67 (0x7c7b0b769d67 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bf5949 (0x7c7b0fff5949 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: <unknown function> + 0x5bf5d01 (0x7c7b0fff5d01 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0x5ba3eeb (0x7c7b0ffa3eeb in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::TCPStore(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10d::TCPStoreOptions const&) + 0x4b5 (0x7c7b0ffa67f5 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xc06325 (0x7c7b1f006325 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0xc385b4 (0x7c7b1f0385b4 in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x37f2dd (0x7c7b1e77f2dd in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #10: /home/hsl/.conda/envs/vllm/bin/python() [0x5429c4]
frame #11: _PyObject_MakeTpCall + 0x30b (0x516d1b in /home/hsl/.conda/envs/vllm/bin/python)
frame #12: /home/hsl/.conda/envs/vllm/bin/python() [0x56af3d]
frame #13: _PyObject_Call + 0x122 (0x554992 in /home/hsl/.conda/envs/vllm/bin/python)
frame #14: /home/hsl/.conda/envs/vllm/bin/python() [0x5519d5]
frame #15: /home/hsl/.conda/envs/vllm/bin/python() [0x5171cb]
frame #16: <unknown function> + 0x37d35b (0x7c7b1e77d35b in /home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/torch/lib/libtorch_python.so)
frame #17: _PyObject_MakeTpCall + 0x30b (0x516d1b in /home/hsl/.conda/envs/vllm/bin/python)
frame #18: _PyEval_EvalFrameDefault + 0x6ce (0x52105e in /home/hsl/.conda/envs/vllm/bin/python)
frame #19: /home/hsl/.conda/envs/vllm/bin/python() [0x58d842]
frame #20: /home/hsl/.conda/envs/vllm/bin/python() [0x543c8c]
frame #21: PyObject_Vectorcall + 0x51 (0x5384c1 in /home/hsl/.conda/envs/vllm/bin/python)
frame #22: _PyEval_EvalFrameDefault + 0x6ce (0x52105e in /home/hsl/.conda/envs/vllm/bin/python)
frame #23: _PyObject_FastCallDictTstate + 0x285 (0x5196e5 in /home/hsl/.conda/envs/vllm/bin/python)
frame #24: /home/hsl/.conda/envs/vllm/bin/python() [0x5517a4]
frame #25: /home/hsl/.conda/envs/vllm/bin/python() [0x5171cb]
frame #26: _PyObject_Call + 0xb5 (0x554925 in /home/hsl/.conda/envs/vllm/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x51fc (0x525b8c in /home/hsl/.conda/envs/vllm/bin/python)
frame #28: PyEval_EvalCode + 0xae (0x5dc01e in /home/hsl/.conda/envs/vllm/bin/python)
frame #29: /home/hsl/.conda/envs/vllm/bin/python() [0x619487]
frame #30: /home/hsl/.conda/envs/vllm/bin/python() [0x613ff7]
frame #31: PyRun_StringFlags + 0x5f (0x6100bf in /home/hsl/.conda/envs/vllm/bin/python)
frame #32: PyRun_SimpleStringFlags + 0x3a (0x60fc5a in /home/hsl/.conda/envs/vllm/bin/python)
frame #33: Py_RunMain + 0x4e1 (0x60d5f1 in /home/hsl/.conda/envs/vllm/bin/python)
frame #34: Py_BytesMain + 0x39 (0x5c4809 in /home/hsl/.conda/envs/vllm/bin/python)
frame #35: <unknown function> + 0x2a1ca (0x7c7b2802a1ca in /lib/x86_64-linux-gnu/libc.so.6)
frame #36: __libc_start_main + 0x8b (0x7c7b2802a28b in /lib/x86_64-linux-gnu/libc.so.6)
frame #37: /home/hsl/.conda/envs/vllm/bin/python() [0x5c4635]

ERROR 08-06 11:04:47 [core.py:632] EngineCore failed to start.
ERROR 08-06 11:04:47 [core.py:632] Traceback (most recent call last):
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
ERROR 08-06 11:04:47 [core.py:632]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 08-06 11:04:47 [core.py:632]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 441, in __init__
ERROR 08-06 11:04:47 [core.py:632]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 77, in __init__
ERROR 08-06 11:04:47 [core.py:632]     self.model_executor = executor_class(vllm_config)
ERROR 08-06 11:04:47 [core.py:632]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
ERROR 08-06 11:04:47 [core.py:632]     self._init_executor()
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 94, in _init_executor
ERROR 08-06 11:04:47 [core.py:632]     self.workers = WorkerProc.wait_for_ready(unready_workers)
ERROR 08-06 11:04:47 [core.py:632]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 08-06 11:04:47 [core.py:632]   File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 446, in wait_for_ready
ERROR 08-06 11:04:47 [core.py:632]     raise e from None
ERROR 08-06 11:04:47 [core.py:632] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 636, in run_engine_core
    raise e
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 623, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 441, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 77, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 94, in _init_executor
    self.workers = WorkerProc.wait_for_ready(unready_workers)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 446, in wait_for_ready
    raise e from None
Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1856, in <module>
    uvloop.run(run_server(args))
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1791, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1811, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 163, in from_vllm_config
    return cls(
           ^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 117, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 98, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 677, in __init__
    super().__init__(
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 697, in launch_core_engines
    wait_for_engine_startup(
  File "/home/hsl/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 750, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
/home/hsl/.conda/envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
